{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85abf5e4",
   "metadata": {},
   "source": [
    "# Notebook Description:\n",
    "\n",
    "The following notebook contains code for processing and cropping .svs files from whole slide images down to 512x512 crops that will be inputted to the trained DLC model for classification.\n",
    "\n",
    "- For TCGA samples the .svs files were downloaded from: https://portal.gdc.cancer.gov/projects/TCGA-THCA\n",
    "\n",
    "- For Nikiforov samples the .svs files were initially processed by accessing the web server svs files hosted on Aperio Image Scope through: http://image.upmc.edu:8080/NikiForov%20EFV%20Study/BoxA/view.apml?listview=1. A script was written to split the image in parralel before saving each crop for futher processing in this notebook.\n",
    "\n",
    "For both datasets, crops of 512x512 were extracted from the whole slide images and then filtered to only contain images patches where the mean and standard deviation of the pixel values were one standard deviation away from that of the TharunThompson reference dataset. \n",
    "\n",
    "From this subset, a random sample of 20 crops was selected to be submitted to the trained classifier for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa3d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, random, tqdm, shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import openslide\n",
    "from openslide import OpenSlideError\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b8f92",
   "metadata": {},
   "source": [
    "# Find Mean and Std Pixel Values of Reference Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89603811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_std_pixel_value(img_list, image_type):\n",
    "    \"\"\"\n",
    "    Finds the mean and std pixel values for a list of images.\n",
    "    \"\"\"   \n",
    "    avg_pixel_value = []\n",
    "    stddev_pixel_value= []\n",
    "    for file in img_list:\n",
    "        image = Image.open(file)\n",
    "        img_arr = np.array(image)\n",
    "        avg = img_arr.mean()\n",
    "        std = img_arr.std()\n",
    "        avg_pixel_value.append(avg)\n",
    "        stddev_pixel_value.append(std)\n",
    "        \n",
    "    avg_pixel_value = np.array(avg_pixel_value)  \n",
    "    stddev_pixel_value = np.array(stddev_pixel_value)\n",
    "        \n",
    "    print(\"Average pixel value for %s images is:\" % image_type, \n",
    "          avg_pixel_value.mean())\n",
    "    print(\"Std of mean values is:\", \n",
    "          avg_pixel_value.std())\n",
    "    print(\"Average std dev of pixel value for %s images is:\" % image_type, \n",
    "          stddev_pixel_value.mean())\n",
    "    print(\"Std of std values is:\", \n",
    "          stddev_pixel_value.std())\n",
    "    \n",
    "    return avg_pixel_value.mean(), avg_pixel_value.std(), \\\n",
    "            stddev_pixel_value.mean(), stddev_pixel_value.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad174501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_imgs/TT_Cond\\\\img00000000.png',\n",
       " 'all_imgs/TT_Cond\\\\img00000001.png',\n",
       " 'all_imgs/TT_Cond\\\\img00000002.png',\n",
       " 'all_imgs/TT_Cond\\\\img00000003.png',\n",
       " 'all_imgs/TT_Cond\\\\img00000004.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting an image list for the TharunThompson 512x512 cropped images:\n",
    "tt_imgs = (glob.glob(\"all_imgs/TT_Cond/*.png\"))\n",
    "tt_imgs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be3a3f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average pixel value for Tharun Thompson images is: 150.08757740333542\n",
      "Std of mean values is: 23.83420922062602\n",
      "Average std dev of pixel value for Tharun Thompson images is: 49.310766475752004\n",
      "Std of std values is: 6.027017945351431\n"
     ]
    }
   ],
   "source": [
    "tt_mean, tt_mean_std, tt_std, tt_std_std = find_mean_std_pixel_value(\n",
    "                                              tt_imgs, 'Tharun Thompson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_mean, tt_mean_std, tt_std, tt_std_std = 150.0875, 23.8342, 49.3107, 6.027"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9685dee",
   "metadata": {},
   "source": [
    "# Loading and Cropping Whole Slide Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9001d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_slide(filename):\n",
    "    \"\"\"\n",
    "    Loads a whole-slide image (*.svs, etc).\n",
    "    Args:\n",
    "    filename: Name of the slide file.\n",
    "    Returns:\n",
    "    An OpenSlide object representing a whole-slide image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        slide = openslide.open_slide(filename)\n",
    "    except OpenSlideError:\n",
    "        slide = None\n",
    "    except FileNotFoundError:\n",
    "        slide = None\n",
    "    \n",
    "    return slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d7e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_wsi(slide_path, crop_size, tile_level, \n",
    "             tile_mean, tile_mean_std, tile_std, tile_std_std, \n",
    "             save_loc, sample_no):\n",
    "    \"\"\"\n",
    "    slide: the whole slide image\n",
    "    crop_size: the size of each crop to perform\n",
    "    tile_level: the level of Deep Zoom corresponding to the resolution desired\n",
    "    tile_mean: image mean pixel value from reference dataset\n",
    "    tile_mean_std: std of the tile means\n",
    "    tile_std: image std of pixel value from reference dataset\n",
    "    tile_std_std: std of the tile stds\n",
    "    sample_no: no. of sample slides to return\n",
    "    \"\"\"\n",
    "     # Return file id from path:\n",
    "    file_id = os.path.splitext((os.path.basename(slide_path)))[0]\n",
    "\n",
    "    # Load the slide:\n",
    "    try:\n",
    "        slide = load_slide(slide_path)\n",
    "    except:\n",
    "        print(\"Corrupt slide: %s.\" % (file_id))\n",
    "        return file_id\n",
    "\n",
    "    # Create a Deep Zoom Generator Object with crop size = tile size:\n",
    "    tiles = DeepZoomGenerator(slide, \n",
    "                              tile_size = crop_size, \n",
    "                              overlap=0,        # zero overlap between crops\n",
    "                              limit_bounds=False)\n",
    "    \n",
    "    # Looking at a specific zoom level:\n",
    "    level_num = min(16, tiles.level_count-1)  \n",
    "\n",
    "    # Extract the number of cols/rows in specified tile level of Deep Zoom:\n",
    "    cols, rows = tiles.level_tiles[level_num]\n",
    "\n",
    "    # Create a list of all tiles that match search criteria (mean/std)\n",
    "    #print(\"Creating tile list...\")\n",
    "    cr_list = []\n",
    "\n",
    "    for row in tqdm(range(rows), desc = 'Row Processing Progress'):\n",
    "        for col in range(cols):\n",
    "            temp_tile = tiles.get_tile(level_num, (col, row))\n",
    "            temp_tile_RGB = temp_tile.convert('RGB')\n",
    "            temp_tile_np = np.array(temp_tile_RGB)\n",
    "            # Calc. pixel mean and std of tile:\n",
    "            t_m = temp_tile_np.mean()\n",
    "            t_s = temp_tile_np.std()\n",
    "\n",
    "            # Setting limits equal to reference dataset 1 std range:\n",
    "            if (t_m > tt_mean - tt_mean_std) and (t_m < tt_mean + tt_mean_std)\\\n",
    "                and (t_s > tt_std - tt_std_std) and (t_s < tt_std + tt_std_std):\n",
    "                cr_list.append((col, row))\n",
    "\n",
    "    print(\"Number of Crops Matching Criteria: \", len(cr_list))\n",
    "\n",
    "    # Selecting random crops = sample_no\n",
    "    sample_no = min(len(cr_list), sample_no)\n",
    "    sel_crops = random.sample(cr_list, sample_no)\n",
    "\n",
    "    # Saving the crops:\n",
    "    for i in range(len(sel_crops)):\n",
    "        # Return the cropped image for each saved tile: \n",
    "        crop = tiles.get_tile(\n",
    "                        level_num, \n",
    "                        sel_crops[i]) # Deep zoom level and address (column, row)\n",
    "\n",
    "        # Convert to RGB and save:\n",
    "        crop_RGB = crop.convert('RGB')\n",
    "        crop_RGB.save(\"%s/%s_%s.jpeg\" % (save_loc, file_id, i))\n",
    "    \n",
    "    print(\"Saved %s cropped images from %s.\" % (sample_no, file_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d182c3",
   "metadata": {},
   "source": [
    "# Processing TCGA Slides:\n",
    "- Processing was performed in batches, below shows the processing of 6 .svs files downloaded from the TCGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7dc1d630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcga\\\\img_crops',\n",
       " 'tcga\\\\TCGA-DE-A2OL.svs',\n",
       " 'tcga\\\\TCGA-DJ-A13W.svs',\n",
       " 'tcga\\\\TCGA-DJ-A13X.svs',\n",
       " 'tcga\\\\TCGA-EM-A2CR.svs']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths of all TCGA WSI's:\n",
    "tcga_paths = glob.glob(\"tcga/*\")\n",
    "tcga_paths[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a5f43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|█████████████████████████████████████████████████████████| 27/27 [05:31<00:00, 12.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  170\n",
      "Saved 20 cropped images from TCGA-DE-A2OL.\n",
      "Processed 1 whole slide images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|█████████████████████████████████████████████████████████| 79/79 [06:04<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  1312\n",
      "Saved 20 cropped images from TCGA-DJ-A13W.\n",
      "Processed 2 whole slide images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|█████████████████████████████████████████████████████████| 77/77 [08:14<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  2577\n",
      "Saved 20 cropped images from TCGA-DJ-A13X.\n",
      "Processed 3 whole slide images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|███████████████████████████████████████████████████████| 107/107 [02:42<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  2514\n",
      "Saved 20 cropped images from TCGA-EM-A2CR.\n",
      "Processed 4 whole slide images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|█████████████████████████████████████████████████████████| 36/36 [03:24<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  14\n",
      "Saved 14 cropped images from TCGA-EM-A3OB.\n",
      "Processed 5 whole slide images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row Processing Progress: 100%|█████████████████████████████████████████████████████████| 78/78 [07:44<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Crops Matching Criteria:  1547\n",
      "Saved 20 cropped images from TCGA-MK-A84Z.\n",
      "Processed 6 whole slide images...\n"
     ]
    }
   ],
   "source": [
    "corrupt_files = []\n",
    "\n",
    "for i in range(len(tcga_paths)):\n",
    "    c_file = crop_wsi(tcga_paths[i], crop_size = 512, tile_level = 16, \n",
    "                  tile_mean = tt_mean, tile_mean_std = tt_mean_std, \n",
    "                  tile_std = tt_std, tile_std_std = tt_std_std,\n",
    "                  save_loc= 'tcga/img_crops', sample_no = 20)\n",
    "    if c_file is not None:\n",
    "        corrupt_files.append(c_file)\n",
    "    print(\"Processed %s whole slide images...\" % (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597f94c",
   "metadata": {},
   "source": [
    "# Processing Nikiforov Slides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bab7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nikiforov(slide_nos, slide_dir, \n",
    "                     tile_mean, tile_mean_std, tile_std, tile_std_std,\n",
    "                     dest_folder, sample_no):\n",
    "    \"\"\"\n",
    "    Filter images extracted from WSIs to only include ones that match\n",
    "    specified criteria (based on ref dataset mean and std). Choose 'X'\n",
    "    crops at random from that subset and save in destination folder.\n",
    "    \n",
    "    slide_nos: a list of slide numbers to process\n",
    "    \"\"\"\n",
    "    for slide in slide_nos:\n",
    "        # Filepaths of all slide pre-cropped images:\n",
    "        fpaths = glob.glob('%s%s/*' % (slide_dir, slide))\n",
    "        \n",
    "        # Create a list of slides that match required criteria:\n",
    "        sl_list = []\n",
    "\n",
    "        for i, file in tqdm(enumerate(fpaths), desc = 'Processing %s Crops' % slide):\n",
    "            image = Image.open(file)\n",
    "            img_arr = np.array(image)\n",
    "            avg = img_arr.mean()\n",
    "            std = img_arr.std()\n",
    "            if (avg > tt_mean - tt_mean_std) and (avg < tt_mean + tt_mean_std)\\\n",
    "                        and (std > tt_std - tt_std_std) and (std < tt_std + tt_std_std):\n",
    "                        sl_list.append(i)\n",
    "        \n",
    "        print(\"Found %s crops matching criteria in Slide %s...\" % (len(sl_list), slide))\n",
    "        \n",
    "        # Selecting random crops = sample_no\n",
    "        sample_no = min(len(sl_list), sample_no)\n",
    "        sel_crops = random.sample(sl_list, sample_no)\n",
    "        \n",
    "        # Return filenames of crops selected\n",
    "        sel_fnames = [fpaths[x] for x in sel_crops]\n",
    "        \n",
    "        # Rename and move selected files to different directory:\n",
    "        for j, file in enumerate(sel_fnames):\n",
    "            new_fname = '%s/%s/NIK-%s_%s.jpeg' % (slide_dir, slide, slide, j)\n",
    "            os.rename(file, new_fname)\n",
    "            shutil.move(new_fname, dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea848d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_nums = ['A102', 'A111', 'A121', 'A123', 'A124', 'A126', 'A127', 'A128', 'A129', 'A130',\n",
    "                'A131', 'A134', 'A136', 'A137', 'A138', 'A026', 'A027', 'A029', 'A035', 'A036', 'A037', \n",
    "                'A041', 'A043', 'A046', 'A047', 'A049', 'A052', 'A056', 'A058', 'A059', 'A060', 'A062', \n",
    "                'A073', 'A079', 'A008', 'A080']\n",
    "slide_dir = \"nikiforov/all_imgs/\"\n",
    "dest = 'nikiforov/img_crops/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0ce505a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing A121 Crops: 11424it [01:44, 109.36it/s]\n",
      "Processing A123 Crops: 7303it [01:06, 109.11it/s]\n",
      "Processing A124 Crops: 11088it [01:41, 109.14it/s]\n",
      "Processing A126 Crops: 12768it [02:04, 102.15it/s]\n",
      "Processing A127 Crops: 10500it [01:41, 103.36it/s]\n",
      "Processing A128 Crops: 9156it [01:30, 101.48it/s]\n",
      "Processing A129 Crops: 9492it [01:32, 102.97it/s]\n",
      "Processing A130 Crops: 11424it [01:49, 104.67it/s]\n",
      "Processing A131 Crops: 11424it [01:45, 107.93it/s]\n",
      "Processing A134 Crops: 9492it [01:27, 108.74it/s]\n",
      "Processing A136 Crops: 12432it [01:50, 112.09it/s]\n",
      "Processing A137 Crops: 7000it [01:07, 103.63it/s]\n",
      "Processing A138 Crops: 10692it [01:36, 110.63it/s]\n",
      "Processing A026 Crops: 9922it [01:39, 99.43it/s] \n",
      "Processing A027 Crops: 9047it [01:31, 98.50it/s] \n",
      "Processing A029 Crops: 6868it [01:06, 103.71it/s]\n",
      "Processing A035 Crops: 7719it [01:13, 104.67it/s]\n",
      "Processing A036 Crops: 13104it [02:02, 106.73it/s]\n",
      "Processing A037 Crops: 7381it [01:10, 104.26it/s]\n",
      "Processing A041 Crops: 7298it [01:11, 102.08it/s]\n",
      "Processing A043 Crops: 6800it [01:06, 101.69it/s]\n",
      "Processing A046 Crops: 11424it [01:48, 104.96it/s]\n",
      "Processing A047 Crops: 9156it [01:28, 103.95it/s]\n",
      "Processing A049 Crops: 12768it [01:59, 107.06it/s]\n",
      "Processing A052 Crops: 9680it [01:30, 107.03it/s]\n",
      "Processing A056 Crops: 7676it [01:15, 101.31it/s]\n",
      "Processing A058 Crops: 12284it [01:53, 107.85it/s]\n",
      "Processing A059 Crops: 9196it [01:30, 101.11it/s]\n",
      "Processing A060 Crops: 6693it [01:06, 100.51it/s]\n",
      "Processing A062 Crops: 14364it [02:19, 102.61it/s]\n",
      "Processing A073 Crops: 8080it [01:18, 103.06it/s]\n",
      "Processing A079 Crops: 4758it [00:44, 107.27it/s]\n",
      "Processing A008 Crops: 6363it [01:02, 102.37it/s]\n",
      "Processing A080 Crops: 10500it [01:46, 98.56it/s] \n"
     ]
    }
   ],
   "source": [
    "process_nikiforov(slide_nos = slide_nums, slide_dir= slide_dir, \n",
    "                    tile_mean = tt_mean, tile_mean_std = tt_mean_std, \n",
    "                    tile_std = tt_std, tile_std_std = tt_std_std,\n",
    "                    dest_folder=dest, sample_no=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9982dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
